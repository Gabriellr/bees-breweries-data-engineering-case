{"block_file": {"custom/artistic_glade.py:custom:python:artistic glade": {"content": "Crie um novo bloco de tipo test ", "file_path": "custom/artistic_glade.py", "language": "python", "type": "custom", "uuid": "artistic_glade"}, "custom/enchanted_potion.py:custom:python:enchanted potion": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@custom\ndef transform_custom(*args, **kwargs):\n    \"\"\"\n    args: The output from any upstream parent blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your custom logic here\n\n    return {}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "custom/enchanted_potion.py", "language": "python", "type": "custom", "uuid": "enchanted_potion"}, "custom/sincere_surf.py:custom:python:sincere surf": {"content": "import smtplib\nfrom email.mime.text import MIMEText\n\nsmtp_host = \"smtp.office365.com\"\nsmtp_port = 587\nusername = \"gabrieltatianarodrigues@hotmail.com\"\npassword = \"copjuygprulgqukm\"\n\nmsg = MIMEText(\"Teste SMTP via Hotmail.\")\nmsg[\"Subject\"] = \"Alerta de Teste\"\nmsg[\"From\"] = username\nmsg[\"To\"] = username\n\ntry:\n    server = smtplib.SMTP(smtp_host, smtp_port)\n    server.starttls()\n    server.login(username, password)\n    server.send_message(msg)\n    server.quit()\n    print(\"\u2705 E-mail enviado com sucesso!\")\nexcept Exception as e:\n    print(\"\u274c Falha ao enviar:\")\n    print(e)\n", "file_path": "custom/sincere_surf.py", "language": "python", "type": "custom", "uuid": "sincere_surf"}, "custom/thrilling_prophecy.py:custom:python:thrilling prophecy": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\nimport os\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Exporta o DataFrame para um arquivo CSV dentro do diret\u00f3rio do projeto Mage AI.\n\n    Exemplo de caminho: <projeto_mage>/data/bronze/breweries_raw.csv\n    \"\"\"\n    # Caminho relativo ao diret\u00f3rio do projeto\n    dir_path = 'data/csv'\n    os.makedirs(dir_path, exist_ok=True)  # Cria o diret\u00f3rio, se n\u00e3o existir\n\n    filepath = os.path.join(dir_path, 'breweries_raw.csv')\n    FileIO().export(df, filepath)", "file_path": "custom/thrilling_prophecy.py", "language": "python", "type": "custom", "uuid": "thrilling_prophecy"}, "data_exporters/brewery_bronze_export.py:data_exporter:python:brewery bronze export": {"content": "# Importa utilit\u00e1rios do Mage.ai para caminho do reposit\u00f3rio e configura\u00e7\u00e3o do S3\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.s3 import S3\nfrom pandas import DataFrame\nfrom os import path\nimport time  # Para controlar tempo entre retentativas\nimport traceback  # Para imprimir rastros de erro completos\n\n# Importa decorador apenas se ainda n\u00e3o tiver sido carregado (usado no ambiente do Mage)\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_s3(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Exporta dados do DataFrame para um bucket S3, com at\u00e9 3 tentativas em caso de falha.\n    A configura\u00e7\u00e3o do acesso ao S3 deve estar definida em 'io_config.yaml'.\n    \"\"\"\n\n    # Caminho do arquivo de configura\u00e7\u00e3o e perfil\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    # Nome do bucket e chave do objeto onde os dados ser\u00e3o salvos\n    bucket_name = 'db-inbev-bronze-layer'\n    object_key = 'breweries_raw.json'\n\n    # At\u00e9 3 tentativas para exportar os dados ao S3\n    max_attempts = 3\n    attempt = 0\n    success = False\n\n    while attempt < max_attempts and not success:\n        try:\n            # Realiza a exporta\u00e7\u00e3o usando a configura\u00e7\u00e3o do Mage.ai\n            S3.with_config(ConfigFileLoader(config_path, config_profile)).export(\n                df,\n                bucket_name,\n                object_key,\n            )\n\n            print(f\"[Sucesso] Dados exportados com sucesso para o bucket: s3://{bucket_name}/{object_key}\")\n            success = True  # Marca a opera\u00e7\u00e3o como bem-sucedida\n\n        except Exception as e:\n            attempt += 1\n            print(f\"[Erro] Tentativa {attempt}/{max_attempts} falhou ao exportar para S3.\")\n            traceback.print_exc()  # Imprime o traceback completo do erro para debug\n            time.sleep(2)  # Aguarda 2 segundos antes de tentar novamente\n\n            # Se falhar nas 3 tentativas, lan\u00e7a o erro final\n            if attempt == max_attempts:\n                raise RuntimeError(f\"[Falha Cr\u00edtica] N\u00e3o foi poss\u00edvel exportar os dados para s3://{bucket_name}/{object_key} ap\u00f3s {max_attempts} tentativas.\") from e", "file_path": "data_exporters/brewery_bronze_export.py", "language": "python", "type": "data_exporter", "uuid": "brewery_bronze_export"}, "data_exporters/cozy_willow.py:data_exporter:python:cozy willow": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n    \"\"\"\n    Exports data to some source.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Output (optional):\n        Optionally return any object and it'll be logged and\n        displayed when inspecting the block run.\n    \"\"\"\n    # Specify your data exporting logic here\n\n\n", "file_path": "data_exporters/cozy_willow.py", "language": "python", "type": "data_exporter", "uuid": "cozy_willow"}, "data_exporters/csv.py:data_exporter:python:csv": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\nimport os\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Exporta o DataFrame para um arquivo CSV dentro do diret\u00f3rio do projeto Mage AI.\n\n    Exemplo de caminho: <projeto_mage>/data/bronze/breweries_raw.csv\n    \"\"\"\n    # Caminho relativo ao diret\u00f3rio do projeto\n    dir_path = 'data/csv'\n    os.makedirs(dir_path, exist_ok=True)  # Cria o diret\u00f3rio, se n\u00e3o existir\n\n    filepath = os.path.join(dir_path, 'breweries_raw.csv')\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/csv.py", "language": "python", "type": "data_exporter", "uuid": "csv"}, "data_exporters/export_breweries_to_csv_local.py:data_exporter:python:export breweries to csv local": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\nimport os\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Exporta o DataFrame para um arquivo CSV dentro do diret\u00f3rio do projeto Mage AI.\n\n    Exemplo de caminho: <projeto_mage>/data/csv/breweries_gold.csv\n    \"\"\"\n    # Caminho relativo ao diret\u00f3rio do projeto\n    dir_path = 'data/csv'\n    os.makedirs(dir_path, exist_ok=True)  # Cria o diret\u00f3rio, se n\u00e3o existir\n\n    filepath = os.path.join(dir_path, 'breweries_gold.csv')\n    FileIO().export(df, filepath)", "file_path": "data_exporters/export_breweries_to_csv_local.py", "language": "python", "type": "data_exporter", "uuid": "export_breweries_to_csv_local"}, "data_exporters/s3_export_raw_gold_parquet.py:data_exporter:python:s3 export raw gold parquet": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.s3 import S3\nfrom pandas import DataFrame\nfrom os import path\nimport logging\nimport time\nfrom datetime import datetime  # necess\u00e1rio para pegar a data de hoje\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_s3(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Exporta DataFrame para um bucket S3 no formato Parquet.\n    Realiza at\u00e9 3 tentativas em caso de erro.\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    # Bucket e caminho do arquivo\n    bucket_name = 'db-inbev-gold-layer'\n    folder_name = datetime.today().strftime('%Y-%m-%d')\n    s3_prefix = f\"{folder_name}\"\n    object_key = f\"{s3_prefix}/breweries_gold.parquet\"\n\n    max_retries = 3\n    attempt = 0\n\n    while attempt < max_retries:\n        try:\n            attempt += 1\n            logging.info(f\" Tentativa {attempt} de exportar para S3...\")\n\n            S3.with_config(ConfigFileLoader(config_path, config_profile)).export(\n                df,\n                bucket_name,\n                object_key,\n            )\n\n            logging.info(f\" Arquivo exportado com sucesso para s3://{bucket_name}/{object_key}\")\n            break  # Sucesso \u2192 encerra loop\n        except Exception as e:\n            logging.error(f\"Erro ao exportar para S3 (tentativa {attempt}): {e}\")\n            if attempt < max_retries:\n                time.sleep(2)  # Aguarda 2 segundos antes da pr\u00f3xima tentativa\n            else:\n                logging.critical(\"Todas as tentativas de exporta\u00e7\u00e3o falharam.\")\n                raise  # Relevanta a exce\u00e7\u00e3o para Mage registrar como erro\n", "file_path": "data_exporters/s3_export_raw_gold_parquet.py", "language": "python", "type": "data_exporter", "uuid": "s3_export_raw_gold_parquet"}, "data_exporters/save_breweries_partitioned.py:data_exporter:python:save breweries partitioned": {"content": "import boto3\nfrom botocore.exceptions import ClientError\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.s3 import S3\nfrom pandas import DataFrame\nfrom os import path\nimport shutil\nimport logging\nimport os\nimport time\nfrom datetime import datetime\nfrom pyspark.sql import SparkSession\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_s3(df: DataFrame, **kwargs) -> None:\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    bucket_name = 'db-inbev-silver-layer'\n    folder_name = datetime.today().strftime('%Y-%m-%d')\n    s3_prefix = f\"breweries_partitioned/{folder_name}\"\n\n    spark = SparkSession.builder.getOrCreate()\n\n    if not hasattr(df, 'repartition'):\n        df = spark.createDataFrame(df)\n\n    local_path = f\"/tmp/{folder_name}_breweries_partitioned\"\n    shutil.rmtree(local_path, ignore_errors=True)\n\n    try:\n        df.repartition(\"country\", \"region\") \\\n          .write \\\n          .partitionBy(\"country\", \"region\") \\\n          .mode(\"overwrite\") \\\n          .parquet(local_path)\n        logging.info(f\"Dados salvos localmente em: {local_path}\")\n    except Exception as e:\n        logging.error(f\"Erro ao salvar dados localmente: {str(e)}\")\n        raise\n\n    # Mage.ai S3 para upload\n    s3 = S3.with_config(ConfigFileLoader(config_path, config_profile))\n\n    # boto3 para manipula\u00e7\u00e3o direta (deletar + enviar _SUCCESS)\n    session = boto3.Session()\n    s3_client = session.client('s3')\n\n    def delete_prefix_objects(bucket, prefix):\n        paginator = s3_client.get_paginator('list_objects_v2')\n        pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n\n        deleted = 0\n        for page in pages:\n            if 'Contents' in page:\n                objects = [{'Key': obj['Key']} for obj in page['Contents']]\n                s3_client.delete_objects(Bucket=bucket, Delete={'Objects': objects})\n                deleted += len(objects)\n        return deleted\n\n    # Limpa prefixo antes de enviar\n    try:\n        logging.info(f\"Deletando arquivos antigos em s3://{bucket_name}/{s3_prefix}/ ...\")\n        deleted_count = delete_prefix_objects(bucket_name, s3_prefix)\n        logging.info(f\"{deleted_count} arquivos deletados.\")\n    except ClientError as e:\n        logging.error(f\"Erro ao deletar arquivos antigos: {e}\")\n        raise\n\n    # Upload dos arquivos parquet via Mage S3 com 3 tentativas\n    for root, _, files in os.walk(local_path):\n        for file in files:\n            if not file.endswith(\".parquet\"):\n                continue\n            full_path = os.path.join(root, file)\n            relative_path = os.path.relpath(full_path, local_path).replace(\"\\\\\", \"/\")\n            s3_key = f\"{s3_prefix}/{relative_path}\"\n\n            for attempt in range(1, 4):\n                try:\n                    logging.info(f\"Enviando: {s3_key} (tentativa {attempt})\")\n                    s3.export(full_path, bucket_name, s3_key)\n                    logging.info(f\"Upload OK: {s3_key}\")\n                    break\n                except Exception as e:\n                    logging.warning(f\"Erro no upload {s3_key} (tentativa {attempt}): {str(e)}\")\n                    if attempt < 3:\n                        time.sleep(2)\n                    else:\n                        logging.error(f\"Falha definitiva no upload de {s3_key}\")\n                        raise\n\n    # Cria e envia arquivo _SUCCESS via boto3 (para evitar erro de formato)\n    success_s3_key = f\"{s3_prefix}/_SUCCESS\"\n    success_marker_path = os.path.join(local_path, \"_SUCCESS\")\n\n    with open(success_marker_path, 'w') as f:\n        f.write('')\n\n    try:\n        with open(success_marker_path, 'rb') as data:\n            s3_client.put_object(Bucket=bucket_name, Key=success_s3_key, Body=data)\n        logging.info(f\"_SUCCESS enviado para s3://{bucket_name}/{success_s3_key}\")\n    except Exception as e:\n        logging.error(f\"Falha ao enviar arquivo _SUCCESS: {str(e)}\")\n        raise\n\n    logging.info(f\"Exporta\u00e7\u00e3o conclu\u00edda com sucesso em s3://{bucket_name}/{s3_prefix}/\")", "file_path": "data_exporters/save_breweries_partitioned.py", "language": "python", "type": "data_exporter", "uuid": "save_breweries_partitioned"}, "data_loaders/delete_buckt.py:data_loader:python:delete buckt": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.s3 import S3\nfrom os import path\nimport logging\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n@data_loader\ndef delete_file_from_s3(*args, **kwargs):\n    \"\"\"\n    Deleta um arquivo espec\u00edfico de um bucket S3 configurado via Mage.io.\n    \"\"\"\n\n    # Caminho para o arquivo de configura\u00e7\u00e3o do Mage\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    # Defina aqui seu bucket e o caminho do objeto a ser deletado\n    bucket_name = 'db-inbev-gold-layer'\n    object_key = kwargs.get('object_key', 'raw_gold.parquet')  # Exemplo: 'raw_gold.parquet'\n\n    logging.info(f\"\ud83d\uddd1\ufe0f Deletando s3://{bucket_name}/{object_key}...\")\n\n    # Inicializa cliente S3 do Mage com config\n    s3 = S3.with_config(ConfigFileLoader(config_path, config_profile))\n\n    # Executa a exclus\u00e3o\n    s3.delete(bucket_name, object_key)\n\n    logging.info(f\"\u2705 Arquivo s3://{bucket_name}/{object_key} deletado com sucesso.\")\n\n    return f\"Arquivo deletado: s3://{bucket_name}/{object_key}\"", "file_path": "data_loaders/delete_buckt.py", "language": "python", "type": "data_loader", "uuid": "delete_buckt"}, "data_loaders/delete_file_bronze.py:data_loader:python:delete file bronze": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nimport boto3\nfrom os import path\nimport logging\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\n@data_loader\ndef delete_all_files_in_s3_bucket(*args, **kwargs):\n    \"\"\"\n    Deleta todos os arquivos (incluindo subdiret\u00f3rios) de um bucket S3.\n    Use o par\u00e2metro 'prefix' se quiser deletar apenas uma pasta (ex: '2025-06-30/').\n    \"\"\"\n\n    bucket_name = 'db-inbev-bronze-layer'\n    prefix = kwargs.get('prefix', '')  # Ex: '' para tudo, ou '2025-06-30/'\n\n    s3_client = boto3.client('s3')\n\n    try:\n        paginator = s3_client.get_paginator('list_objects_v2')\n        pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n\n        deleted_files = 0\n        for page in pages:\n            if 'Contents' not in page:\n                logging.info(\"Nenhum arquivo para deletar.\")\n                continue\n\n            for obj in page['Contents']:\n                key = obj['Key']\n                logging.info(f\"Deletando: {key}\")\n                s3_client.delete_object(Bucket=bucket_name, Key=key)\n                deleted_files += 1\n\n        logging.info(f\"Dele\u00e7\u00e3o conclu\u00edda: {deleted_files} arquivos removidos de s3://{bucket_name}/{prefix}\")\n        return f\"{deleted_files} arquivos deletados de s3://{bucket_name}/{prefix}\"\n\n    except Exception as e:\n        logging.error(f\"Erro ao deletar arquivos do bucket: {str(e)}\")\n        raise", "file_path": "data_loaders/delete_file_bronze.py", "language": "python", "type": "data_loader", "uuid": "delete_file_bronze"}, "data_loaders/delete_file_gold.py:data_loader:python:delete file gold": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nimport boto3\nfrom os import path\nimport logging\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\n@data_loader\ndef delete_all_files_in_s3_bucket(*args, **kwargs):\n    \"\"\"\n    Deleta todos os arquivos (incluindo subdiret\u00f3rios) de um bucket S3.\n    Use o par\u00e2metro 'prefix' se quiser deletar apenas uma pasta (ex: '2025-06-30/').\n    \"\"\"\n\n    bucket_name = 'db-inbev-gold-layer'\n    prefix = kwargs.get('prefix', '')  # Ex: '' para tudo, ou '2025-06-30/'\n\n    s3_client = boto3.client('s3')\n\n    try:\n        paginator = s3_client.get_paginator('list_objects_v2')\n        pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n\n        deleted_files = 0\n        for page in pages:\n            if 'Contents' not in page:\n                logging.info(\"Nenhum arquivo para deletar.\")\n                continue\n\n            for obj in page['Contents']:\n                key = obj['Key']\n                logging.info(f\"Deletando: {key}\")\n                s3_client.delete_object(Bucket=bucket_name, Key=key)\n                deleted_files += 1\n\n        logging.info(f\"\u2705 Dele\u00e7\u00e3o conclu\u00edda: {deleted_files} arquivos removidos de s3://{bucket_name}/{prefix}\")\n        return f\"{deleted_files} arquivos deletados de s3://{bucket_name}/{prefix}\"\n\n    except Exception as e:\n        logging.error(f\"Erro ao deletar arquivos do bucket: {str(e)}\")\n        raise", "file_path": "data_loaders/delete_file_gold.py", "language": "python", "type": "data_loader", "uuid": "delete_file_gold"}, "data_loaders/delete_file_gold_1.py:data_loader:python:delete file gold 1": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nimport boto3\nfrom os import path\nimport logging\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\n@data_loader\ndef delete_all_files_in_s3_bucket(*args, **kwargs):\n    \"\"\"\n    Deleta todos os arquivos (incluindo subdiret\u00f3rios) de um bucket S3.\n    Use o par\u00e2metro 'prefix' se quiser deletar apenas uma pasta (ex: '2025-06-30/').\n    \"\"\"\n\n    bucket_name = 'db-inbev-gold-layer'\n    prefix = kwargs.get('prefix', '')  # Ex: '' para tudo, ou '2025-06-30/'\n\n    s3_client = boto3.client('s3')\n\n    try:\n        paginator = s3_client.get_paginator('list_objects_v2')\n        pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n\n        deleted_files = 0\n        for page in pages:\n            if 'Contents' not in page:\n                logging.info(\"Nenhum arquivo para deletar.\")\n                continue\n\n            for obj in page['Contents']:\n                key = obj['Key']\n                logging.info(f\"Deletando: {key}\")\n                s3_client.delete_object(Bucket=bucket_name, Key=key)\n                deleted_files += 1\n\n        logging.info(f\"Dele\u00e7\u00e3o conclu\u00edda: {deleted_files} arquivos removidos de s3://{bucket_name}/{prefix}\")\n        return f\"{deleted_files} arquivos deletados de s3://{bucket_name}/{prefix}\"\n\n    except Exception as e:\n        logging.error(f\"Erro ao deletar arquivos do bucket: {str(e)}\")\n        raise", "file_path": "data_loaders/delete_file_gold_1.py", "language": "python", "type": "data_loader", "uuid": "delete_file_gold_1"}, "data_loaders/delete_file_silver.py:data_loader:python:delete file silver": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nimport boto3\nfrom os import path\nimport logging\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\n@data_loader\ndef delete_all_files_in_s3_bucket(*args, **kwargs):\n    \"\"\"\n    Deleta todos os arquivos (incluindo subdiret\u00f3rios) de um bucket S3.\n    Use o par\u00e2metro 'prefix' se quiser deletar apenas uma pasta (ex: '2025-06-30/').\n    \"\"\"\n\n    bucket_name = 'db-inbev-silver-layer'\n    prefix = kwargs.get('prefix', '')  # Ex: '' para tudo, ou '2025-06-30/'\n\n    s3_client = boto3.client('s3')\n\n    try:\n        paginator = s3_client.get_paginator('list_objects_v2')\n        pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n\n        deleted_files = 0\n        for page in pages:\n            if 'Contents' not in page:\n                logging.info(\"Nenhum arquivo para deletar.\")\n                continue\n\n            for obj in page['Contents']:\n                key = obj['Key']\n                logging.info(f\"Deletando: {key}\")\n                s3_client.delete_object(Bucket=bucket_name, Key=key)\n                deleted_files += 1\n\n        logging.info(f\"Dele\u00e7\u00e3o conclu\u00edda: {deleted_files} arquivos removidos de s3://{bucket_name}/{prefix}\")\n        return f\"{deleted_files} arquivos deletados de s3://{bucket_name}/{prefix}\"\n\n    except Exception as e:\n        logging.error(f\"Erro ao deletar arquivos do bucket: {str(e)}\")\n        raise", "file_path": "data_loaders/delete_file_silver.py", "language": "python", "type": "data_loader", "uuid": "delete_file_silver"}, "data_loaders/friendly_rogue.py:data_loader:python:friendly rogue": {"content": "import smtplib\nfrom email.mime.text import MIMEText\n\nsmtp_host = \"smtp.office365.com\"\nsmtp_port = 587\nusername = \"gabrieltatianarodrigues@hotmail.com\"\npassword = \"copjuygprulgqukm\"\n\nmsg = MIMEText(\"Teste SMTP via Hotmail.\")\nmsg[\"Subject\"] = \"Alerta de Teste\"\nmsg[\"From\"] = username\nmsg[\"To\"] = username\n\ntry:\n    server = smtplib.SMTP(smtp_host, smtp_port)\n    server.starttls()\n    server.login(username, password)\n    server.send_message(msg)\n    server.quit()\n    print(\"\u2705 E-mail enviado com sucesso!\")\nexcept Exception as e:\n    print(\"\u274c Falha ao enviar:\")\n    print(e)\n", "file_path": "data_loaders/friendly_rogue.py", "language": "python", "type": "data_loader", "uuid": "friendly_rogue"}, "data_loaders/import_data_api.py:data_loader:python:import data api": {"content": "# Importa os decoradores apenas se ainda n\u00e3o estiverem carregados (necess\u00e1rio no ambiente do Mage.ai)\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport requests\nimport time\nfrom pyspark.sql import SparkSession\n\n@data_loader\ndef load_data_from_api_spark(*args, **kwargs):\n    url = \"https://api.openbrewerydb.org/v1/breweries\"\n    all_data, page = [], 1\n\n    # Loop para pagina\u00e7\u00e3o da API\n    while True:\n        attempt = 0\n        success = False\n        # Tenta at\u00e9 3 vezes fazer a requisi\u00e7\u00e3o da p\u00e1gina\n        while attempt < 3 and not success:\n            try:\n                response = requests.get(f\"{url}?per_page=200&page={page}\", timeout=10)\n                response.raise_for_status()  # Lan\u00e7a exce\u00e7\u00e3o para status HTTP de erro\n                data = response.json()\n                success = True  # Se chegou aqui, a requisi\u00e7\u00e3o foi bem-sucedida\n            except requests.RequestException as e:\n                attempt += 1\n                print(f\"Tentativa {attempt}/3 falhou para p\u00e1gina {page}: {e}\")\n                time.sleep(2)  # Aguarda 2 segundos antes de tentar novamente\n                if attempt == 3:\n                    raise RuntimeError(f\"Erro ap\u00f3s 3 tentativas ao buscar dados da p\u00e1gina {page}\") from e\n\n        # Se n\u00e3o houver mais dados na API, interrompe o loop\n        if not data:\n            break\n\n        all_data.extend(data)\n        page += 1\n\n    # Cria uma SparkSession para processar os dados como DataFrame distribu\u00eddo\n    spark = SparkSession.builder.appName(\"MageDataLoad\").getOrCreate()\n    df_spark = spark.createDataFrame(all_data)\n\n    # Converte o DataFrame Spark para Pandas, mais compat\u00edvel com Mage.ai\n    df_pandas = df_spark.toPandas()\n\n    spark.stop()  # Libera os recursos da SparkSession\n    return df_pandas\n\n# Fun\u00e7\u00e3o de teste que valida se o DataFrame resultante tem ao menos uma linha\n@test\ndef test_row_count(df, *args) -> None:\n    assert len(df.index) >= 1, 'Verificar se os dados possuem linhas suficientes'", "file_path": "data_loaders/import_data_api.py", "language": "python", "type": "data_loader", "uuid": "import_data_api"}, "data_loaders/load_breweries_from_s3_layer_bronze.py:data_loader:python:load breweries from s3 layer bronze": {"content": "# Importa utilit\u00e1rios do Mage.ai para configura\u00e7\u00e3o do projeto e acesso ao S3\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.s3 import S3\nfrom os import path\nimport time  # Para pausa entre tentativas\n\n# Importa os decoradores apenas se ainda n\u00e3o estiverem carregados (\u00fatil no Mage.ai)\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_from_s3_bucket(*args, **kwargs):\n    \"\"\"\n    Carrega dados do S3 definidos em um bucket e chave espec\u00edficos.\n    Realiza at\u00e9 3 tentativas em caso de erro com delay entre elas.\n    A configura\u00e7\u00e3o de acesso deve estar em 'io_config.yaml'.\n    \"\"\"\n\n    # Caminho do arquivo de configura\u00e7\u00e3o e perfil do Mage\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    # Bucket e objeto (caminho do arquivo) no S3\n    bucket_name = 'db-inbev-bronze-layer'\n    object_key = 'breweries_raw.json'\n\n    # Par\u00e2metros para controle de tentativas\n    max_retries = 3\n    delay_seconds = 5  # tempo entre tentativas em segundos\n\n    last_exception = None  # Guarda a \u00faltima exce\u00e7\u00e3o para relan\u00e7ar se necess\u00e1rio\n\n    for attempt in range(1, max_retries + 1):\n        try:\n            # Tenta carregar os dados do S3\n            print(f\"Tentando carregar dados do S3 (tentativa {attempt}/{max_retries})...\")\n            df = S3.with_config(ConfigFileLoader(config_path, config_profile)).load(\n                bucket_name,\n                object_key,\n            )\n            print(f\"[Sucesso] Dados carregados de s3://{bucket_name}/{object_key}\")\n            return df  # Sucesso: retorna o DataFrame\n        except Exception as e:\n            # Em caso de falha, exibe o erro e espera antes de tentar novamente\n            last_exception = e\n            print(f\"[Erro] Tentativa {attempt} falhou ao carregar do S3: {e}\")\n            if attempt < max_retries:\n                print(f\"Aguardando {delay_seconds} segundos antes da pr\u00f3xima tentativa...\")\n                time.sleep(delay_seconds)\n            else:\n                # \u00daltima tentativa falhou: levanta erro cr\u00edtico\n                print(\"[Falha cr\u00edtica] Todas as tentativas de leitura falharam.\")\n                raise RuntimeError(f\"Erro ao carregar dados do S3 ap\u00f3s {max_retries} tentativas.\") from e\n\n    # Seguran\u00e7a: relan\u00e7a a \u00faltima exce\u00e7\u00e3o (n\u00e3o deveria chegar aqui)\n    raise last_exception\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Teste para garantir que os dados foram carregados corretamente.\n    Suporta tanto DataFrame Pandas quanto Spark.\n    \"\"\"\n    assert output is not None, 'O output est\u00e1 vazio'\n\n    import pandas as pd\n    if isinstance(output, pd.DataFrame):\n        assert not output.empty, 'O DataFrame Pandas est\u00e1 vazio'\n    else:\n        # Se n\u00e3o for Pandas, assume que \u00e9 Spark\n        assert output.count() > 0, 'O DataFrame Spark est\u00e1 vazio'\n", "file_path": "data_loaders/load_breweries_from_s3_layer_bronze.py", "language": "python", "type": "data_loader", "uuid": "load_breweries_from_s3_layer_bronze"}, "data_loaders/load_breweries_from_s3_layer_silver.py:data_loader:python:load breweries from s3 layer silver": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.s3 import S3\nfrom os import path\nimport boto3\nimport pandas as pd\nfrom datetime import datetime\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_from_s3_bucket(*args, **kwargs):\n    \"\"\"\n    L\u00ea todos arquivos .parquet de um bucket S3, incluindo subpastas,\n    e concatena os DataFrames usando boto3 + Mage S3 loader.\n    Verifica se o arquivo _SUCCESS existe antes de carregar os dados.\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    bucket_name = 'db-inbev-silver-layer'\n\n    # Define prefixo com a data atual\n    folder_name = datetime.today().strftime('%Y-%m-%d')\n    s3_prefix = f\"breweries_partitioned/{folder_name}/\"\n\n    # Inicializa boto3 para listar arquivos\n    s3_client = boto3.client('s3')\n\n    # Verifica se arquivo _SUCCESS existe\n    success_key = s3_prefix + '_SUCCESS'\n\n    try:\n        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=success_key)\n        success_exists = 'Contents' in response and any(obj['Key'] == success_key for obj in response['Contents'])\n    except Exception as e:\n        raise RuntimeError(f\"Erro ao verificar arquivo _SUCCESS no bucket: {str(e)}\")\n\n    if not success_exists:\n        raise FileNotFoundError(f\"Arquivo _SUCCESS n\u00e3o encontrado em s3://{bucket_name}/{success_key}\")\n\n    # Lista arquivos .parquet\n    paginator = s3_client.get_paginator('list_objects_v2')\n    pages = paginator.paginate(Bucket=bucket_name, Prefix=s3_prefix)\n\n    object_keys = []\n    for page in pages:\n        for obj in page.get('Contents', []):\n            key = obj['Key']\n            if key.endswith('.parquet'):\n                object_keys.append(key)\n\n    if not object_keys:\n        raise Exception(f\"Nenhum arquivo .parquet encontrado em s3://{bucket_name}/{s3_prefix}\")\n\n    print(f\" {len(object_keys)} arquivos encontrados para carregar...\")\n\n    # Carrega todos os arquivos usando Mage S3\n    dfs = []\n    for key in object_keys:\n        print(f\" Lendo: {key}\")\n        df = S3.with_config(ConfigFileLoader(config_path, config_profile)).load(bucket_name, key)\n        dfs.append(df)\n\n    return pd.concat(dfs, ignore_index=True)\n\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, ' Nenhum dado carregado.'\n    print(f\" DataFrame final com {len(output)} registros.\")\n", "file_path": "data_loaders/load_breweries_from_s3_layer_silver.py", "language": "python", "type": "data_loader", "uuid": "load_breweries_from_s3_layer_silver"}, "data_loaders/load_virtualized_parquet.py:data_loader:python:load virtualized parquet": {"content": "from mage_ai.data_preparation.decorators import data_loader\nfrom pyspark.sql import SparkSession\nimport logging\n\n@data_loader\ndef load_virtualized_parquet(*args, **kwargs):\n    \"\"\"\n    Cria uma view tempor\u00e1ria no Spark com base em arquivos Parquet no S3.\n    Permite que os dados sejam consultados via Spark SQL como uma tabela virtual.\n    \"\"\"\n\n    # Nome do bucket S3 e prefixo (subpasta opcional)\n    bucket_name = 'db-inbev-gold-layer'\n    prefix = kwargs.get('folder_name', '')  # Ex: '2025-06-29/' ou ''\n\n    # Caminho completo no S3\n    s3_path = f\"s3a://{bucket_name}/{prefix}*\"\n\n    # Inicia a sess\u00e3o Spark\n    spark = SparkSession.builder \\\n        .appName(\"VirtualizeParquet\") \\\n        .getOrCreate()\n\n    logging.info(f\"\ud83d\udce5 Lendo arquivos Parquet de {s3_path}\")\n\n    # L\u00ea os dados e registra uma view tempor\u00e1ria\n    df = spark.read.parquet(s3_path)\n    df.createOrReplaceTempView(\"breweries_virtual_view\")\n\n    logging.info(\"\u2705 View tempor\u00e1ria criada: breweries_virtual_view\")\n\n    return df", "file_path": "data_loaders/load_virtualized_parquet.py", "language": "python", "type": "data_loader", "uuid": "load_virtualized_parquet"}, "data_loaders/shy_sword.py:data_loader:python:shy sword": {"content": "from mage_ai.data_preparation.decorators import data_loader\n\n@data_loader\ndef load_data(*args, **kwargs):\n    return {\"mensagem\": \"Bloco reconhecido com sucesso\"}\n\n    # Nome do bucket S3 e prefixo (subpasta opcional)\n    bucket_name = 'db-inbev-gold-layer'\n    prefix = kwargs.get('folder_name', '')  # Ex: '2025-06-29/' ou ''\n\n    # Caminho completo no S3\n    s3_path = f\"s3a://{bucket_name}/{prefix}*\"\n\n    # Inicia a sess\u00e3o Spark\n    spark = SparkSession.builder \\\n        .appName(\"VirtualizeParquet\") \\\n        .getOrCreate()\n\n    logging.info(f\"\ud83d\udce5 Lendo arquivos Parquet de {s3_path}\")\n\n    # L\u00ea os dados e registra uma view tempor\u00e1ria\n    df = spark.read.parquet(s3_path)\n    df.createOrReplaceTempView(\"breweries_virtual_view\")\n\n    logging.info(\"\u2705 View tempor\u00e1ria criada: breweries_virtual_view\")\n\n    return {\"status\": \"ok\"}", "file_path": "data_loaders/shy_sword.py", "language": "python", "type": "data_loader", "uuid": "shy_sword"}, "data_loaders/virtual.py:data_loader:python:virtual": {"content": "from mage_ai.data_preparation.decorators import data_loader\nfrom pyspark.sql import SparkSession\nimport logging\n\n@data_loader\ndef load_virtualized_parquet(*args, **kwargs):\n    \"\"\"\n    Cria uma view tempor\u00e1ria no Spark com base em arquivos Parquet no S3.\n    Permite que os dados sejam consultados via Spark SQL como uma tabela virtual.\n    \"\"\"\n\n    bucket_name = 'db-inbev-gold-layer'\n    prefix = kwargs.get('folder_name', '')  # Ex: '2025-06-29/' ou ''\n    s3_path = f\"s3a://{bucket_name}/{prefix}*\"\n\n    logging.info(f\"\ud83d\udce5 Virtualizando arquivos Parquet: {s3_path}\")\n\n    spark = SparkSession.builder \\\n        .appName(\"VirtualizeParquet\") \\\n        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.520\") \\\n        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\") \\\n        .getOrCreate()\n\n    try:\n        df = spark.read.parquet(s3_path)\n        df.createOrReplaceTempView(\"breweries_virtual_view\")\n        logging.info(\"\u2705 View tempor\u00e1ria criada com nome: breweries_virtual_view\")\n        return df\n    except Exception as e:\n        logging.error(f\"\u274c Erro ao virtualizar arquivos Parquet: {e}\")\n        raise\n", "file_path": "data_loaders/virtual.py", "language": "python", "type": "data_loader", "uuid": "virtual"}, "transformers/breweries_transformer_silver.py:transformer:python:breweries transformer silver": {"content": "from mage_ai.data_cleaner.transformer_actions.base import BaseAction\nfrom mage_ai.data_cleaner.transformer_actions.constants import ActionType, Axis\nfrom mage_ai.data_cleaner.transformer_actions.utils import build_transformer_action\nfrom pandas import DataFrame\nimport numpy as np\nimport logging\nimport time\n\n# Verifica se est\u00e1 no ambiente Mage e importa os decoradores necess\u00e1rios\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@transformer\ndef execute_transformer_action(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Transforma o DataFrame:\n    - Remove duplicatas com BaseAction\n    - Concatena address_2 + address_3\n    - Faz coalesce entre colunas redundantes\n    - Retorna um DataFrame com colunas selecionadas\n    \"\"\"\n\n    max_retries = 3  # N\u00famero de tentativas\n    for attempt in range(1, max_retries + 1):\n        try:\n            logging.info(f\"Tentativa {attempt}: iniciando transforma\u00e7\u00e3o do DataFrame.\")\n\n            # \ud83d\udd39 Remove duplicatas com BaseAction\n            action = build_transformer_action(\n                df,\n                action_type=ActionType.DROP_DUPLICATE,\n                arguments=df.columns,\n                axis=Axis.ROW,\n                options={'keep': 'first'},\n            )\n            df_cleaned = BaseAction(action).execute(df)\n\n            # \ud83d\udd39 Concatena address_2 e address_3 em uma \u00fanica coluna\n            df_cleaned['address_2'] = np.where(\n                df_cleaned[['address_2', 'address_3']].notnull().any(axis=1),\n                df_cleaned[['address_2', 'address_3']].fillna('').agg(' '.join, axis=1),\n                None\n            )\n\n            # \ud83d\udd39 Coalesce: preenche region com state_province ou state\n            df_cleaned['region'] = df_cleaned['state_province'].combine_first(df_cleaned['state'])\n\n            # \ud83d\udd39 Coalesce: preenche address_1 com street\n            df_cleaned['address_1'] = df_cleaned['address_1'].combine_first(df_cleaned['street'])\n\n            # \ud83d\udd39 C\u00f3pias auxiliares para agrega\u00e7\u00f5es futuras\n            df_cleaned['region_copy'] = df_cleaned['region']\n            df_cleaned['country_copy'] = df_cleaned['country']\n\n            # \ud83d\udd39 Seleciona as colunas finais\n            cols_final = [\n                'id', 'name', 'brewery_type', 'country', 'country_copy', 'region', 'region_copy', 'city',\n                'postal_code', 'address_1', 'address_2', 'longitude',\n                'latitude', 'phone', 'website_url'\n            ]\n            df_result = df_cleaned[cols_final]\n\n            logging.info(\"Transforma\u00e7\u00e3o conclu\u00edda com sucesso.\")\n            return df_result\n\n        except Exception as e:\n            logging.warning(f\"Erro ao transformar os dados na tentativa {attempt}: {str(e)}\")\n            if attempt < max_retries:\n                time.sleep(2)  # Aguarda antes de tentar novamente\n            else:\n                logging.error(\"Todas as tentativas de transforma\u00e7\u00e3o falharam.\")\n                raise e  # Interrompe o pipeline no Mage\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Testa se o DataFrame transformado tem conte\u00fado.\n    \"\"\"\n    assert output is not None and len(output) > 0, 'O output est\u00e1 vazio ou indefinido'\n    print(f\" Transforma\u00e7\u00e3o gerou {len(output)} registros.\")", "file_path": "transformers/breweries_transformer_silver.py", "language": "python", "type": "transformer", "uuid": "breweries_transformer_silver"}, "transformers/trtansform_breweries_aggregated_view.py:transformer:python:trtansform breweries aggregated view": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Realiza agrega\u00e7\u00e3o em pandas.DataFrame por tipo, pa\u00eds e regi\u00e3o.\n    \"\"\"\n    \"\"\"\n    Renomeia 'country_copy' para 'country' e 'region_copy': 'region', faz agrega\u00e7\u00e3o.\n    \"\"\"\n    data = data.rename(columns={\n    'country_copy': 'country',\n    'region_copy': 'region',\n    })\n    # Agrupa e conta o n\u00famero de breweries\n    brewery_agg_df = (\n        data.groupby(['brewery_type','country','region'])\n            .agg(brewery_count=('id', 'count'))\n            .reset_index()\n    )\n    \n    return brewery_agg_df\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'O resultado da transforma\u00e7\u00e3o est\u00e1 vazio.'\n    print(f\" Transforma\u00e7\u00e3o conclu\u00edda: {len(output)} linhas agregadas.\")", "file_path": "transformers/trtansform_breweries_aggregated_view.py", "language": "python", "type": "transformer", "uuid": "trtansform_breweries_aggregated_view"}, "pipelines/elt_proj_breweries/metadata.yaml:pipeline:yaml:elt proj breweries/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - brewery_bronze_export\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: import_data_api\n  retry_config: null\n  status: failed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: import_data_api\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - load_breweries_from_s3_layer_bronze\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: brewery_bronze_export\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - import_data_api\n  uuid: brewery_bronze_export\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_path: data_loaders/load_breweries_from_s3_layer_bronze.py\n    file_source:\n      path: data_loaders/load_breweries_from_s3_layer_bronze.py\n  downstream_blocks:\n  - breweries_transformer_silver\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_breweries_from_s3_layer_bronze\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks:\n  - brewery_bronze_export\n  uuid: load_breweries_from_s3_layer_bronze\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_path: transformers/breweries_transformer_silver.py\n    file_source:\n      path: transformers/breweries_transformer_silver.py\n  downstream_blocks:\n  - save_breweries_partitioned\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: breweries_transformer_silver\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - load_breweries_from_s3_layer_bronze\n  uuid: breweries_transformer_silver\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - load_breweries_from_s3_layer_silver\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: save_breweries_partitioned\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - breweries_transformer_silver\n  uuid: save_breweries_partitioned\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_path: data_loaders/load_breweries_from_s3_layer_silver.py\n    file_source:\n      path: data_loaders/load_breweries_from_s3_layer_silver.py\n  downstream_blocks:\n  - trtansform_breweries_aggregated_view\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_breweries_from_s3_layer_silver\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks:\n  - save_breweries_partitioned\n  uuid: load_breweries_from_s3_layer_silver\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_path: transformers/trtansform_breweries_aggregated_view.py\n    file_source:\n      path: transformers/trtansform_breweries_aggregated_view.py\n  downstream_blocks:\n  - s3_export_raw_gold_parquet\n  - export_breweries_to_csv_local\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: trTansform_breweries_aggregated_view\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - load_breweries_from_s3_layer_silver\n  uuid: trtansform_breweries_aggregated_view\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_path: data_exporters/s3_export_raw_gold_parquet.py\n    file_source:\n      path: data_exporters/s3_export_raw_gold_parquet.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: s3_export_raw_gold_parquet\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - trtansform_breweries_aggregated_view\n  uuid: s3_export_raw_gold_parquet\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_path: data_exporters/export_breweries_to_csv_local.py\n    file_source:\n      path: data_exporters/export_breweries_to_csv_local.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_breweries_to_csv_local\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - trtansform_breweries_aggregated_view\n  uuid: export_breweries_to_csv_local\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-06-29 16:19:10.254298+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: elt_proj_breweries\nnotification_config: {}\nremote_variables_dir: null\nretry_config:\n  delay: 1\n  retries: 1\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: elt_proj_breweries\nvariables_dir: /home/src/mage_data/Mage_projeto\nwidgets: []\n", "file_path": "pipelines/elt_proj_breweries/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "elt_proj_breweries/metadata"}, "pipelines/elt_proj_breweries/__init__.py:pipeline:python:elt proj breweries/  init  ": {"content": "", "file_path": "pipelines/elt_proj_breweries/__init__.py", "language": "python", "type": "pipeline", "uuid": "elt_proj_breweries/__init__"}, "/home/src/Mage_projeto/transformers/transform_breweries_aggregated_view.py:transformer:python:home/src/Mage projeto/transformers/transform breweries aggregated view": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Realiza agrega\u00e7\u00e3o em pandas.DataFrame por tipo, pa\u00eds e regi\u00e3o.\n    \"\"\"\n    \"\"\"\n    Renomeia 'country_copy' para 'country' e 'region_copy': 'region', faz agrega\u00e7\u00e3o.\n    \"\"\"\n    data = data.rename(columns={\n    'country_copy': 'country',\n    'region_copy': 'region',\n    })\n    # Agrupa e conta o n\u00famero de breweries\n    brewery_agg_df = (\n        data.groupby(['brewery_type','country','region'])\n            .agg(brewery_count=('id', 'count'))\n            .reset_index()\n    )\n    \n    return brewery_agg_df\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'O resultado da transforma\u00e7\u00e3o est\u00e1 vazio.'\n    print(f\" Transforma\u00e7\u00e3o conclu\u00edda: {len(output)} linhas agregadas.\")", "file_path": "/home/src/Mage_projeto/transformers/transform_breweries_aggregated_view.py", "language": "python", "type": "transformer", "uuid": "transform_breweries_aggregated_view"}}, "custom_block_template": {"custom_templates/blocks/f:markdown:markdown": {"block_type": "markdown", "color": null, "configuration": null, "description": null, "language": "markdown", "name": null, "pipeline": {}, "tags": [], "user": {}, "template_uuid": "f", "uuid": "custom_templates/blocks/f", "content": ""}}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}