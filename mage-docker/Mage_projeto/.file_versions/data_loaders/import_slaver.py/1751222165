from mage_ai.settings.repo import get_repo_path
from mage_ai.io.config import ConfigFileLoader
from os import path
import boto3
from botocore.exceptions import ClientError
from pyspark.sql import SparkSession
from datetime import datetime

if 'data_loader' not in globals():
    from mage_ai.data_preparation.decorators import data_loader
if 'test' not in globals():
    from mage_ai.data_preparation.decorators import test


@data_loader
def load_from_s3_bucket(*args, **kwargs):
    config_path = path.join(get_repo_path(), 'io_config.yaml')
    config_profile = 'default'

    bucket_name = 'db-inbev-silver-layer'

    # Usa data de hoje para definir o prefixo
    folder_name = datetime.today().strftime('%Y-%m-%d')
    s3_prefix = f"breweries_raw/{folder_name}"

    s3_client = boto3.client('s3')

    success_key = f"{s3_prefix}/_SUCCESS"

    try:
        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=success_key)
        success_exists = 'Contents' in response and any(obj['Key'] == success_key for obj in response['Contents'])
    except ClientError as e:
        raise RuntimeError(f"Erro ao verificar _SUCCESS no bucket: {str(e)}")

    if not success_exists:
        raise FileNotFoundError(f"Arquivo _SUCCESS não encontrado em s3://{bucket_name}/{success_key}")

    spark = SparkSession.builder.getOrCreate()

    s3_path = f"s3a://{bucket_name}/{s3_prefix}"

    df = spark.read.parquet(s3_path)

    return df


@test
def test_output(output, *args) -> None:
    assert output is not None and output.count() > 0, 'O DataFrame Spark está vazio'
    print(f"✅ DataFrame carregado com {output.count()} registros.")