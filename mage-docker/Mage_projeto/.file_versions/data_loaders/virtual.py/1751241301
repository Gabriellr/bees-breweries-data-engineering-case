from mage_ai.data_preparation.decorators import data_loader
from pyspark.sql import SparkSession
import logging

@data_loader
def load_virtualized_parquet(*args, **kwargs):
    """
    Cria uma view tempor√°ria no Spark com base em arquivos Parquet no S3.
    Permite que os dados sejam consultados via Spark SQL como uma tabela virtual.
    """

    # Nome do bucket S3
    bucket_name = 'db-inbev-gold-layer'

    # Par√¢metro opcional 'folder_name' define a pasta (ex: 2025-06-29/)
    prefix = kwargs.get('folder_name', '')  # Se n√£o for passado, l√™ tudo
    s3_path = f"s3a://{bucket_name}/{prefix}*"

    logging.info(f"üì• Virtualizando arquivos Parquet: {s3_path}")

    # Inicializa uma SparkSession (se j√° existir, reutiliza)
    spark = SparkSession.builder \
        .appName("VirtualizeParquet") \
        .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.520") \
        .config("spark.hadoop.fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain") \
        .getOrCreate()

    try:
        # L√™ os arquivos Parquet do S3
        df = spark.read.parquet(s3_path)

        # Cria view tempor√°ria com nome padr√£o
        df.createOrReplaceTempView("breweries_virtual_view")
        logging.info("‚úÖ View tempor√°ria criada com nome: breweries_virtual_view")

        return df

    except Exception as e:
        logging.error(f"‚ùå Erro ao virtualizar arquivos Parquet: {e}")
        raise
