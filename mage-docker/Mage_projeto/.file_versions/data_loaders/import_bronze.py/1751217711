from mage_ai.settings.repo import get_repo_path
from mage_ai.io.config import ConfigFileLoader
from mage_ai.io.s3 import S3
from os import path
import time

if 'data_loader' not in globals():
    from mage_ai.data_preparation.decorators import data_loader
if 'test' not in globals():
    from mage_ai.data_preparation.decorators import test


@data_loader
def load_from_s3_bucket(*args, **kwargs):
    config_path = path.join(get_repo_path(), 'io_config.yaml')
    config_profile = 'default'

    bucket_name = 'db-inbev-bronze-layer'
    object_key = 'breweries_raw.json'

    max_retries = 3
    delay_seconds = 5

    last_exception = None

    for attempt in range(1, max_retries + 1):
        try:
            df = S3.with_config(ConfigFileLoader(config_path, config_profile)).load(
                bucket_name,
                object_key,
            )
            return df  # sucesso, retorna o DataFrame
        except Exception as e:
            last_exception = e
            print(f"Tentativa {attempt} falhou: {e}")
            if attempt < max_retries:
                print(f"Repetindo em {delay_seconds} segundos...")
                time.sleep(delay_seconds)
            else:
                print("Número máximo de tentativas atingido. Erro ao carregar dados do S3.")
                raise

    # Se chegar aqui (não deve), lança última exceção
    raise last_exception


@test
def test_output(output, *args) -> None:
    assert output is not None, 'O output está vazio'

    import pandas as pd
    if isinstance(output, pd.DataFrame):
        assert not output.empty, 'O DataFrame pandas está vazio'
    else:
        # Spark DataFrame
        assert output.count() > 0, 'O DataFrame Spark está vazio'