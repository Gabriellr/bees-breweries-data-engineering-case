from mage_ai.settings.repo import get_repo_path
from mage_ai.io.config import ConfigFileLoader
from mage_ai.io.s3 import S3
from pandas import DataFrame
from os import path
import shutil
import logging
import os
import time
from datetime import datetime
from pyspark.sql import SparkSession

# Garante que o decorador do Mage esteja dispon√≠vel
if 'data_exporter' not in globals():
    from mage_ai.data_preparation.decorators import data_exporter

@data_exporter
def export_data_to_s3(df: DataFrame, **kwargs) -> None:
    """
    Exporta DataFrame para o bucket S3, particionando por 'country' e 'region',
    com PySpark e upload via Mage.ai. Inclui:
    - Tratamento de erros
    - 3 tentativas por arquivo
    - Limpeza do prefixo S3 antes do upload
    - Cria√ß√£o de _SUCCESS marker
    """

    # Configura√ß√µes do Mage.ai
    config_path = path.join(get_repo_path(), 'io_config.yaml')
    config_profile = 'default'

    bucket_name = 'db-inbev-silver-layer'
    folder_name = datetime.today().strftime('%Y-%m-%d')
    s3_prefix = f"breweries_raw/{folder_name}"

    # Inicializa sess√£o Spark
    spark = SparkSession.builder.getOrCreate()

    # Converte para Spark DataFrame se ainda for pandas
    if not hasattr(df, 'repartition'):
        df = spark.createDataFrame(df)

    # Define caminho local tempor√°rio
    local_path = f"/tmp/{folder_name}_breweries_raw"
    shutil.rmtree(local_path, ignore_errors=True)  # Limpa antes de salvar

    # Salva particionado localmente
    try:
        df.repartition("country", "region") \
          .write \
          .partitionBy("country", "region") \
          .mode("overwrite") \
          .parquet(local_path)

        logging.info(f"Dados salvos localmente em: {local_path}")
    except Exception as e:
        logging.error(f"Erro ao salvar dados localmente: {str(e)}")
        raise

    # Inicializa o cliente S3
    s3 = S3.with_config(ConfigFileLoader(config_path, config_profile))

    # üîÅ Limpa o prefixo no S3 antes de novos uploads
    try:
        logging.info(f"Removendo arquivos antigos de s3://{bucket_name}/{s3_prefix}/ ...")
        objects_to_delete = s3.list_objects(bucket_name, prefix=s3_prefix)
        if objects_to_delete:
            for obj in objects_to_delete:
                s3.delete(bucket_name, obj)
            logging.info(f"{len(objects_to_delete)} arquivos removidos de {s3_prefix}/.")
        else:
            logging.info(f"Nenhum arquivo antigo encontrado em {s3_prefix}/.")
    except Exception as e:
        logging.error(f"Erro ao limpar prefixo {s3_prefix}/: {str(e)}")
        raise

    # Faz upload dos arquivos .parquet
    for root, _, files in os.walk(local_path):
        for file in files:
            if not file.endswith(".parquet"):
                continue

            full_path = os.path.join(root, file)
            relative_path = os.path.relpath(full_path, local_path).replace("\\", "/")
            s3_key = f"{s3_prefix}/{relative_path}"

            max_retries = 3
            for attempt in range(1, max_retries + 1):
                try:
                    logging.info(f"Enviando: {s3_key} (tentativa {attempt})")
                    s3.export(full_path, bucket_name, s3_key)
                    logging.info(f"Upload conclu√≠do: {s3_key}")
                    break
                except Exception as e:
                    logging.warning(f"Erro ao enviar {s3_key} (tentativa {attempt}): {str(e)}")
                    if attempt < max_retries:
                        time.sleep(2)
                    else:
                        logging.error(f"Falha definitiva ao enviar {s3_key}")
                        raise

    # Cria e envia _SUCCESS marker
    success_s3_key = f"{s3_prefix}/_SUCCESS"
    success_marker_path = os.path.join(local_path, "_SUCCESS")
    open(success_marker_path, 'w').close()  # Cria arquivo vazio localmente

    try:
        # Se j√° existir, remove o antigo
        if s3.exists(bucket_name, success_s3_key):
            logging.info(f"_SUCCESS j√° existe, deletando anterior...")
            s3.delete(bucket_name, success_s3_key)

        s3.export(success_marker_path, bucket_name, success_s3_key)
        logging.info(f"Arquivo _SUCCESS enviado com sucesso para s3://{bucket_name}/{success_s3_key}")
    except Exception as e:
        logging.warning(f"Erro ao criar/enviar _SUCCESS: {str(e)}")

    logging.info(f"Todos os arquivos foram enviados com sucesso para s3://{bucket_name}/{s3_prefix}/")
