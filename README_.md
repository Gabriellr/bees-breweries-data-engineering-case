## BEES Data Engineering ‚Äì Breweries Case

Este pipeline de dados, desenvolvido em Docker, realiza a extra√ß√£o de informa√ß√µes da API Open Brewery DB. Utilizando Apache Spark para processamento e transforma√ß√£o, e Mage.ai para orquestra√ß√£o, ele aplica a arquitetura Medallion para estruturar os dados em tr√™s camadas.

Os dados s√£o armazenados em um Data Lake na nuvem (AWS S3 ). Na camada Bronze, os dados brutos da API s√£o salvos em formato Parquet; na camada Silver, s√£o transformados e particionados por localiza√ß√£o; e na camada Gold, os dados s√£o agregados e enriquecidos com m√©tricas como o n√∫mero de cervejarias por tipo e regi√£o.

# Tecnologias e Ferramentas

- **Mage.ai** ‚Äì Orquestra√ß√£o do pipeline  
- **PySpark** ‚Äì Processamento distribu√≠do  
- **Pandas** ‚Äì Manipula√ß√£o de dados leves  
- **AWS S3** ‚Äì Camadas do data lake  
- **boto3** ‚Äì Integra√ß√£o program√°tica com S3  
- **Docker** ‚Äì Containeriza√ß√£o do ambiente  
    
 ##  Arquitetura de Pipeline

O pipeline foi desenvolvido com base na arquitetura **Medallion**, utilizando tecnologias modernas, escal√°veis e com foco em boas pr√°ticas de engenharia de dados.

###  Estrutura em Camadas (Medallion)

A arquitetura organiza os dados em tr√™s camadas principais:

- **üîπ Bronze (Raw)**  
  Armazena os dados brutos coletados diretamente da API, sem transforma√ß√µes. Serve como fonte de verdade imut√°vel.

- **üî∏ Silver (Curated)**  
  Realiza a limpeza, normaliza√ß√£o e enriquecimento dos dados. Salva em formato colunar (Parquet), particionado por pa√≠s e regi√£o.

- **ü•á Gold (Analytics)**  
  Dados prontos para consumo anal√≠tico. Aqui, s√£o geradas **agrega√ß√µes por tipo de cervejaria e localiza√ß√£o**, otimizando performance para dashboards e relat√≥rios.

![bees-brew drawio](https://github.com/user-attachments/assets/30379ef6-8a66-4e1f-bc69-9505c81358cc)

## Instala√ß√£o

### Pr√©-requisitos

- Docker instalado na sua m√°quina (vers√£o recomendada >= 20.x)

### Passos para rodar o projeto

### Passos para rodar o projeto

1. **Clone o reposit√≥rio**

   Clone este projeto em sua m√°quina local:

   ```bash
   git clone https://github.com/Gabriellr/bees-breweries-data-engineering-case.git
   cd bees-breweries-data-engineering-case
   cd mage-docker

2. **Instale e inicie o Docker**
   Certifique-se de que o Docker Desktop est√° instalado e em execu√ß√£o na sua m√°quina.
Com o Docker pronto, navegue at√© a raiz do projeto mage-docker e execute o seguinte comando para iniciar o Mage.ai:

   ```bash
   docker-compose up --build

 3. **Executando o Pipeline pela Interface do Mage.ai**

Para executar manualmente seu pipeline no Mage.ai, siga os passos abaixo:

  1. Acesse a interface web em: [http://localhost:6789](http://localhost:6789) ou [http://localhost:6789/pipelines/elt_proj_breweries/triggers](http://localhost:6789/pipelines/elt_proj_breweries/triggers) 

  2. No menu lateral, clique na aba **Pipelines**.

  3. Selecione o pipeline desejado (por exemplo: `elt_proj_breweries`).

  4. Na p√°gina do pipeline, v√° at√© a aba **Triggers**.

  5. Clique no bot√£o **`Run@once`** para iniciar a execu√ß√£o manual do pipeline.
![mage6](https://github.com/user-attachments/assets/6a29ce5c-736d-4517-9004-7835fe192dff)



Voc√™ poder√° **acompanhar o progresso**, **verificar os logs** e **analisar os resultados em tempo real** diretamente pela interface gr√°fica.

---
### Melhoria Recomendada: Observabilidade e Governan√ßa com AWS

O pipeline foi desenvolvido com ferramentas gratuitas e de c√≥digo aberto (`Mage.ai`, `PySpark`, `Docker` e `AWS S3`), priorizando acessibilidade e f√°cil reprodutibilidade. No entanto, para ambientes corporativos ou produtivos, recomenda-se a ado√ß√£o de servi√ßos gerenciados da AWS que ampliam significativamente a **observabilidade**, **governan√ßa** e **efici√™ncia operacional**:

- Monitoramento & Alertas:
  Utilizar o **Amazon CloudWatch** em conjunto com o **Amazon SNS** para monitorar falhas, tempos de execu√ß√£o e criar alertas autom√°ticos em tempo real.

- Cat√°logo de Dados:
  Integrar o **AWS Glue Data Catalog** para registrar metadados das camadas Bronze, Silver e Gold, promovendo descoberta, versionamento e auditoria de dados.

- Virtualiza√ß√£o de Dados:  
  Adotar o **Amazon Athena** para consultas SQL diretamente sobre os dados armazenados no S3, sem a necessidade de ETL ou cargas adicionais.

- Qualidade de Dados:  
  Incorporar ferramentas como **AWS Deequ** (Spark) ou **Amazon DataZone** para rastrear qualidade, integridade e regras de neg√≥cio nos dados de forma automatizada.
  
> *Esses recursos n√£o foram utilizados nesta vers√£o do projeto para manter o custo zero, mas s√£o altamente recomendados em contextos com maior volume, criticidade e requisitos de compliance e escalabilidade.*
